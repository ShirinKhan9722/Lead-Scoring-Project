{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "#importing the data set\n",
    "Lead_Score=pd.read_csv(\"Leads.csv\",low_memory=False )\n",
    "Lead_Score.head()\n",
    "\n",
    "Lead_Score.columns\n",
    "\n",
    "Lead_Score.shape\n",
    "\n",
    "Lead_Score.info()\n",
    "\n",
    "Lead_Score.isnull().sum()\n",
    "\n",
    "Lead_Score.describe()\n",
    "\n",
    "Lead_Score.drop(columns=['Lead Number'], axis=1, inplace=True)\n",
    "\n",
    "Lead_Score.head()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "Lead_Score.head()\n",
    "\n",
    "Lead_Score['Get updates on DM Content'].unique()\n",
    "\n",
    "Lead_Score.drop(columns=['Get updates on DM Content'], axis=1, inplace=True)\n",
    "\n",
    "Lead_Score=Lead_Score.replace('Select', np.nan)\n",
    "\n",
    "round(100*(Lead_Score.isnull().sum()/len(Lead_Score.index)), 2)\n",
    "\n",
    "Lead_Score = Lead_Score.drop(Lead_Score.loc[:,list(round(100*(Lead_Score.isnull().sum()/len(Lead_Score.index)), 2)>70)].columns, 1)\n",
    "\n",
    "Lead_Score.head()\n",
    "\n",
    "Lead_Score.shape\n",
    "\n",
    "# **Treating Missing Value**\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Lead_Score['Lead Quality'].describe()\n",
    "\n",
    "sns.countplot(Lead_Score['Lead Quality'])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "Lead_Score['Lead Quality'] = Lead_Score['Lead Quality'].replace(np.nan, 'Not Sure')\n",
    "\n",
    "sns.countplot(Lead_Score['Lead Quality'])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "Lead_Score['Lead Origin'].describe()\n",
    "\n",
    "sns.countplot(Lead_Score['Lead Origin'])\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "fig, axis=plt.subplots(2,2, figsize=(10,8))\n",
    "plot1=sns.countplot(Lead_Score[ 'Asymmetrique Activity Index'],ax=axis[0,0])\n",
    "plot2=sns.countplot(Lead_Score[ 'Asymmetrique Profile Index'],ax=axis[0,1]) \n",
    "plot3=sns.boxplot(Lead_Score['Asymmetrique Activity Score'],ax=axis[1,0]) \n",
    "plot4=sns.boxplot(Lead_Score['Asymmetrique Profile Score'],ax=axis[1,1])\n",
    "plt.tight_layout()\n",
    "\n",
    "Lead_Score[['Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score']].describe()\n",
    "\n",
    "Lead_Score[['Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score']].isnull().sum()\n",
    "\n",
    "More than 45% null values in the above columns therefore we drop it\n",
    "\n",
    "Lead_Score=Lead_Score.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score'],axis=1)\n",
    "\n",
    "Lead_Score.head()\n",
    "\n",
    "round(100*(Lead_Score.isnull().sum()/len(Lead_Score.index)))\n",
    "\n",
    "Lead_Score.City.describe()\n",
    "\n",
    "Lead_Score.City.isnull().sum()\n",
    "\n",
    "sns.countplot(Lead_Score.City)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "#More than 50% of the traffic is from mumbai\n",
    "Lead_Score['City']=Lead_Score['City'].replace(np.nan,'Mumbai')\n",
    "\n",
    "Lead_Score.City.isnull().sum()\n",
    "\n",
    "sns.countplot(Lead_Score.Specialization)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "Lead_Score['Specialization']=Lead_Score['Specialization'].replace(np.nan,'others')\n",
    "\n",
    "Lead_Score.Tags.describe()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.countplot(Lead_Score.Tags)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['Tags']=Lead_Score['Tags'].replace(np.nan,'Will revert after reading the email')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.countplot(Lead_Score.Country)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['Country'].isnull().sum()\n",
    "\n",
    "Lead_Score['Country']=Lead_Score['Country'].replace(np.nan,'India')\n",
    "\n",
    "round(100*(Lead_Score.isnull().sum()/len(Lead_Score.index)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.countplot(Lead_Score['What is your current occupation'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['What is your current occupation'].describe()\n",
    "\n",
    "Lead_Score['What is your current occupation']=Lead_Score['What is your current occupation'].replace(np.nan,'Unemployed')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.countplot(Lead_Score['What matters most to you in choosing a course'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['What matters most to you in choosing a course'].describe()\n",
    "\n",
    "Lead_Score['What matters most to you in choosing a course']=Lead_Score['What matters most to you in choosing a course'].replace(np.nan,'Better Career Prospects')\n",
    "\n",
    "Lead_Score.dropna(inplace=True)\n",
    "\n",
    "round(100*(Lead_Score.isnull().sum()/len(Lead_Score.index)))\n",
    "\n",
    "# **EDA**\n",
    "\n",
    "Conversion_Percent=sum(Lead_Score['Converted'])/len(Lead_Score['Converted'])*100\n",
    "Conversion_Percent\n",
    "\n",
    "sns.countplot(x='Lead Origin',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=75)\n",
    "plt.plot()\n",
    "\n",
    "1. Landing page submission and API are higher in number but thelead conversion rate is quite low(30-35%).\n",
    "2. Lead Add Form are low in number but the conversion rate of the leads are as high as 90%.\n",
    "3. Lead Import are very very low in number not much important.\n",
    "\n",
    "**Conclusion:** We need to focus on how we can improve the conversion rtae of API and Landing Page Submission in order to attain a high conversion rate.\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.countplot(x='Lead Source',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['Lead Source'].unique()\n",
    "\n",
    "Lead_Score['Lead Source']=Lead_Score['Lead Source'].replace(['google'],'Google')\n",
    "Lead_Score['Lead Source']=Lead_Score['Lead Source'].replace(['Facebook', 'blog', 'Pay per Click Ads', 'bing', 'Social Media','WeLearn', 'Click2call', 'Live Chat', 'welearnblog_Home','youtubechannel', 'testone', 'Press_Release', 'NC_EDM'],'Other_Sources')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.countplot(x='Lead Source',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead Source\n",
    "1. Most of the Leads are coming from Olark Chat, Directly and with Google Search, Organic Search.\n",
    "2. Conversion rate of Reference and wellingak Website are high than other Sources.\n",
    "\n",
    "Conclusion: To improve the conversion rate We should Focus on improving the process of lead conversionfrom Olark Chat, Google Search, Direct Traffic, Organic Search. Also, we should try to generate more leads from reference and wellingak Website.\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "sns.countplot(x='Do Not Email',hue='Converted', data=Lead_Score, ax=axs[0])\n",
    "sns.countplot(x='Do Not Call',hue='Converted', data=Lead_Score, ax=axs[1])\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['TotalVisits'].describe()\n",
    "\n",
    "sns.boxplot(data=Lead_Score['TotalVisits'])\n",
    "\n",
    "Percentile=Lead_Score['TotalVisits'].quantile([0.05,0.95]).values\n",
    "\n",
    "#As there are outliers in Total visit we will cap from 5 to 95 percentile where most of the data points lies\n",
    "Lead_Score['TotalVisits'][Lead_Score['TotalVisits']<=Percentile[0]]=Percentile[0]\n",
    "Lead_Score['TotalVisits'][Lead_Score['TotalVisits']>=Percentile[1]]=Percentile[1]\n",
    "\n",
    "print(Percentile[0],\",\",Percentile[1])\n",
    "\n",
    "sns.boxplot(y='TotalVisits',x='Converted', data=Lead_Score )\n",
    "\n",
    "Conclusion: Total Visit are not of much importance as the median of both converted and not converted lies equal.\n",
    "\n",
    "sns.boxplot(y='Total Time Spent on Website',x='Converted', data=Lead_Score )\n",
    "\n",
    "Conclusion: Total Time spent on website is an important factor to note and the leads converted are spending lot of time on the website. Hence website should be more engaging in order to get high conversion rates.\n",
    "\n",
    "sns.boxplot(y='Page Views Per Visit',x='Converted', data=Lead_Score )\n",
    "\n",
    "Conclusion: Page views of both Converted and not converted are almost same hence page views are also not an important factor\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Last Activity',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['Last Activity'].unique()\n",
    "\n",
    "Lead_Score['Last Activity']=Lead_Score['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked','Visited Booth in Tradeshow','Approached upfront','Resubscribed to emails', 'Email Received', 'Email Marked Spam'],'Other_Activity')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Last Activity',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "plt.plot()\n",
    "\n",
    "1. Coversion rate with last activity as SMS sent has a high conversion rate\n",
    "2. The last Activity of the leads as Email Opened are more in number. Hence we can focus on email marketing to increase brand awareness and product details. \n",
    "\n",
    "Lead_Score['Country'].describe()\n",
    "\n",
    "More than 95% of the traffic is from India therefore no such inference can be drwan from Country.\n",
    "\n",
    "Lead_Score['Specialization'].unique()\n",
    "\n",
    "Lead_Score['Specialization']=Lead_Score['Specialization'].replace(['others'],'Other_Specialization')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Specialization',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Conclusion: Specialization is an important factor for conversion rate.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='What is your current occupation',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "1. Working Professionals have high conversion rate therefore the focus should be more on converting leads who are working.\n",
    "2. Unemployed are high in number but the conversion rate is very low, to increase the overall conversion rate we should focus on converting unemployed leads\n",
    "\n",
    "Lead_Score['What matters most to you in choosing a course'].describe()\n",
    "\n",
    "99% leads wants better career prospect therefore no inference can be drawn as such\n",
    "\n",
    "Lead_Score['Search'].describe()                                        \n",
    "        \n",
    "\n",
    "Lead_Score['Magazine'].describe()  \n",
    "\n",
    "Lead_Score['Newspaper Article'].describe() \n",
    "\n",
    "Lead_Score['Newspaper Article'].describe() \n",
    "\n",
    "Lead_Score['X Education Forums'].describe() \n",
    "\n",
    "Lead_Score['Newspaper'].describe() \n",
    "\n",
    "Lead_Score['Digital Advertisement'].describe() \n",
    "\n",
    "Lead_Score['Through Recommendations'].describe() \n",
    "\n",
    "Lead_Score['Receive More Updates About Our Courses'].describe() \n",
    "\n",
    "For below variables all most all the entries are NO therefore no such inference can be drawn.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "'Search', 'Magazine','Newspaper Article', 'X Education Forums', \n",
    "'Newspaper','Digital Advertisement', 'Through Recommendations',\n",
    "'Receive More Updates About Our Courses'\n",
    "\n",
    "Lead_Score['Tags'].describe() \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Tags',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "print(Lead_Score['Tags'].unique())\n",
    "\n",
    "Lead_Score['Tags']=Lead_Score['Tags'].replace(['in touch with EINS','Diploma holder (Not Eligible)','Graduation in progress','opp hangup','Still Thinking','Lost to Others', 'Shall take in the next coming month', 'Lateral student','Interested in Next batch' ,'Recognition issue (DEC approval)','Want to take admission but has financial problems','University not recognized'],'Other_tags')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Tags',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Lead Quality',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead quality is an important metric. As it tells the type of lead.\n",
    "\n",
    "sns.countplot(x= 'Update me on Supply Chain Content',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score.columns\n",
    "\n",
    "sns.countplot(x= 'City',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['City'].describe() \n",
    "\n",
    "Most leads are from Mumbai but the conversion rate is low.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.countplot(x='Last Notable Activity',hue='Converted', data=Lead_Score)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "plt.plot()\n",
    "\n",
    "Lead_Score['I agree to pay the amount through cheque'].describe() \n",
    "\n",
    "Lead_Score['A free copy of Mastering The Interview'].describe() \n",
    "\n",
    "A free copy of Mastering The Interview and I agree to pay the amount through cheque have only No. therefore the inference is irrelevant.\n",
    "\n",
    "# **conclusion**\n",
    "Based on the Analysis we have seen that many columns are not important for the model and it will not contribute to additional information drop them for further analysis.\n",
    "\n",
    "Lead_Score=Lead_Score.drop(['What matters most to you in choosing a course','Search','Magazine','Newspaper Article','X Education Forums','Newspaper',\n",
    "           'Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content','I agree to pay the amount through cheque','A free copy of Mastering The Interview','Country'],1)\n",
    "\n",
    "Lead_Score.head()\n",
    "\n",
    "Lead_Score.shape\n",
    "\n",
    "## **DATA PREPARATION**\n",
    "\n",
    "Converting some binary variables (Yes/No) to 0/1\n",
    "\n",
    "#list of variables to map \n",
    "varlist=['Do Not Email', 'Do Not Call']\n",
    "\n",
    "#defining the map function\n",
    "def binary_map(x):\n",
    "    return x.map({'Yes':1,'No':0})\n",
    "\n",
    "#applying the function to varlist\n",
    "Lead_Score[varlist]=Lead_Score[varlist].apply(binary_map)\n",
    "\n",
    "For categorical variables with multiple levels, create dummy features (one-hot encoded)\n",
    "\n",
    "#creating dummy variable for some  of the categorical variable and dropping the first one\n",
    "dummy=pd.get_dummies(Lead_Score[['Lead Origin','Lead Source','Last Activity', 'Specialization', 'Tags','Lead Quality','City', 'What is your current occupation','Last Notable Activity']], drop_first=True)\n",
    "dummy.head()\n",
    "\n",
    "Lead_Score=pd.concat([Lead_Score,dummy], axis=1)\n",
    "Lead_Score.head()\n",
    "\n",
    "Lead_Score=Lead_Score.drop(['Lead Origin','Lead Source','Last Activity', 'Specialization', 'Tags','Lead Quality','City', 'What is your current occupation','Last Notable Activity'], axis=1)\n",
    "\n",
    "Lead_Score.head()\n",
    "\n",
    "Lead_Score.shape\n",
    "\n",
    "Lead_Score = Lead_Score.drop(['Do Not Call'],1)\n",
    "\n",
    "Lead_Score.head()\n",
    "\n",
    "# Train and Test \n",
    "\n",
    "# Putting feature variable to X\n",
    "X = Lead_Score.drop(['Prospect ID','Converted'], axis=1)\n",
    "# Putting response variable to y\n",
    "y = Lead_Score['Converted']\n",
    "\n",
    "print(y)\n",
    "\n",
    "X.head()\n",
    "\n",
    "\n",
    "#importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Splitting the data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "### Checking the Conversion Rate\n",
    "\n",
    "print(\"Conversion rate is \", (sum(Lead_Score['Converted'])/len(Lead_Score['Converted'].index))*100)\n",
    "\n",
    "# Correlation between different numerical variables for both the Converted and not-converted cases\n",
    "conv_corr = Lead_Score.corr()\n",
    "\n",
    "# Unstacking the correlation matrix to find out top correlations\n",
    "conv_corr_unstacked = conv_corr.unstack().sort_values(kind=\"quicksort\")\n",
    "conv_corr.where(np.triu(np.ones(conv_corr.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Dropping highly correlated features\n",
    "# Last Notable Activity_SMS Sent seems relvant feature might the customer is interested therefore not dropping it \n",
    "X_test = X_test.drop(['Last Notable Activity_Unsubscribed','Last Notable Activity_Email Opened','Last Notable Activity_Unreachable','Last Notable Activity_Email Link Clicked','Last Notable Activity_Page Visited on Website'], 1)\n",
    "X_train = X_train.drop(['Last Notable Activity_Unsubscribed','Last Notable Activity_Email Opened','Last Notable Activity_Unreachable','Last Notable Activity_Email Link Clicked','Last Notable Activity_Page Visited on Website'], 1)\n",
    "\n",
    "conv_corr = X_train.corr()\n",
    "\n",
    "conv_corr.where(np.triu(np.ones(conv_corr.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False).head(10)\n",
    "\n",
    "# MODEL BUILDING\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Logistic regression model\n",
    "\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()\n",
    "\n",
    "#feature selection using RFE\n",
    "# Starting with 15 features selected by RFE\n",
    "# We will then optimize the model further by inspecting VIF and p-value of the features\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=15)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))\n",
    "\n",
    "col = X_train.columns[rfe.support_]\n",
    "col\n",
    "\n",
    "#Assessing the model with stasmodel\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()\n",
    "\n",
    "# Getting the predicted values on the train set\n",
    "y_train_pred = res.predict(X_train_sm)\n",
    "y_train_pred[:10]\n",
    "\n",
    "y_train_pred = y_train_pred.values.reshape(-1)\n",
    "y_train_pred[:10]\n",
    "\n",
    "#Creating df with predicted probabilities \n",
    "y_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\n",
    "y_train_pred_final['Pros_ID'] = y_train.index\n",
    "y_train_pred_final.head()\n",
    "\n",
    "# Creating new column 'predicted' with 1 if Convert_Prob > 0.5 else 0\n",
    "\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Let's see the head\n",
    "y_train_pred_final.head()\n",
    "\n",
    "print(\"Accuracy score\", metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))\n",
    "\n",
    "# Checking VIF's\n",
    "\n",
    "def calculate_vif(X_train):\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df['Features'] = X_train.columns\n",
    "    vif_df['Variance Inflation Factor'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "    vif_df['Variance Inflation Factor'] = round(vif_df['Variance Inflation Factor'], 2)\n",
    "    vif_df = vif_df.sort_values(by = 'Variance Inflation Factor', ascending = False)\n",
    "    print(vif_df)\n",
    "\n",
    "calculate_vif(X_train[col])\n",
    "\n",
    "col = col.drop('Tags_invalid number')\n",
    "col\n",
    "\n",
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm.fit()\n",
    "res.summary()\n",
    "\n",
    "y_train_pred = res.predict(X_train_sm).values.reshape(-1)\n",
    "y_train_pred_final['Convert_Prob'] = y_train_pred\n",
    "\n",
    "# Creating new column 'predicted' with 1 if Convert_Prob > 0.5 else 0\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final.head()\n",
    "\n",
    "# Let's check the overall accuracy.\n",
    "print(\"Accuracy score\", metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))\n",
    "\n",
    "#Checking VIF's again\n",
    "calculate_vif(X_train[col])\n",
    "\n",
    "# The above model is now stable as the VIF values and p-values are good\n",
    "\n",
    "# function name : evaluate_model\n",
    "# argumet : y_true, y_predicted\n",
    "# prints Confusion matrix, accuracy, Sensitivity, Specificity, False Positive Rate, Positive Predictive Value\n",
    "# returns accuracy, Sensitivity, Specificity\n",
    "\n",
    "def evaluate_model(y_true, y_predicted, print_score=False):\n",
    "    confusion = metrics.confusion_matrix(y_true, y_predicted)\n",
    "    # Predicted     not_converted    converted\n",
    "    # Actual\n",
    "    # not_converted        TN         FP\n",
    "    # converted            FN         TP\n",
    "\n",
    "    TP = confusion[1,1] # true positive \n",
    "    TN = confusion[0,0] # true negatives\n",
    "    FP = confusion[0,1] # false positives\n",
    "    FN = confusion[1,0] # false negatives\n",
    "\n",
    "    accuracy_sc = metrics.accuracy_score(y_true, y_predicted)\n",
    "    sensitivity_score = TP / float(TP+FN)\n",
    "    specificity_score = TN / float(TN+FP)\n",
    "    precision_sc = precision_score(y_true, y_predicted)\n",
    "    \n",
    "    if print_score:\n",
    "        print(\"Confusion Matrix :\\n\", confusion)\n",
    "        print(\"Accuracy :\", accuracy_sc)\n",
    "        print(\"Sensitivity :\", sensitivity_score)\n",
    "        print(\"Specificity :\", specificity_score)\n",
    "        print(\"Precision :\", precision_sc)\n",
    "        \n",
    "    return accuracy_sc, sensitivity_score, specificity_score, precision_sc\n",
    "\n",
    "# Evaluating model\n",
    "evaluate_model(y_train_pred_final.Convert, y_train_pred_final.predicted, print_score=True)\n",
    "\n",
    "# Plotting ROC curve\n",
    "\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Convert, y_train_pred_final.Convert_Prob, drop_intermediate = False )\n",
    "\n",
    "draw_roc(y_train_pred_final.Convert, y_train_pred_final.Convert_Prob)\n",
    "\n",
    "#finding optimal values of the cut-off\n",
    "# Predicting Convert status with different probability cutoffs\n",
    "\n",
    "for i in [float(x)/10 for x in range(10)]:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()\n",
    "\n",
    "# Calculating accuracy, sensitivity and specificity for various probability cutoffs from 0.1 to 0.9.\n",
    "\n",
    "df = pd.DataFrame(columns = ['probability_score','accuracy_score','sensitivity_score','specificity_score','precision_score'])\n",
    "\n",
    "for i in [float(x)/10 for x in range(10)]:\n",
    "    (accuracy_score,sensitivity_score,specificity_score,precision_sc) = evaluate_model(y_train_pred_final.Convert, y_train_pred_final[i])\n",
    "    df.loc[i] =[i,accuracy_score,sensitivity_score,specificity_score,precision_sc]\n",
    "\n",
    "df\n",
    "\n",
    "df.plot.line(x='probability_score', y=['accuracy_score','sensitivity_score','specificity_score'])\n",
    "plt.show()\n",
    "\n",
    "#Precision-Recall\n",
    "p, r, thresholds = precision_recall_curve(y_train_pred_final.Convert, y_train_pred_final.Convert_Prob)\n",
    "\n",
    "plt.plot(thresholds, p[:-1], \"g-\")\n",
    "plt.plot(thresholds, r[:-1], \"r-\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.title(\"Precision-Recall Trade off\")\n",
    "plt.show()\n",
    "\n",
    "Comment:\n",
    "\n",
    "In Sensitivity-Specificity-Accuracy plot 0.27 probability looks optimal. In Precision-Recall Curve 0.3 looks optimal.\n",
    "\n",
    "We are taking 0.27 is the optimum point as a cutoff probability and assigning Lead Score in training data.\n",
    "\n",
    "y_train_pred_final = y_train_pred_final.iloc[:, :3]\n",
    "y_train_pred_final['Convert_predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.27 else 0)\n",
    "\n",
    "y_train_pred_final['Lead_Score'] = y_train_pred_final.Convert_Prob.map(lambda x: round(x*100))\n",
    "y_train_pred_final.head()\n",
    "\n",
    "# Evaluating model performance on training data\n",
    "\n",
    "evaluate_model(y_train_pred_final.Convert, y_train_pred_final.Convert_predicted, print_score=True)\n",
    "\n",
    "# Getting the predicted values on the train set\n",
    "X_test_sm = sm.add_constant(X_test[col])\n",
    "y_test_pred = res.predict(X_test_sm)\n",
    "\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred, columns=[\"Converting_Probability\"])\n",
    "y_test_df['Prospect ID'] = y_test_df.index\n",
    "\n",
    "y_predicted_final = pd.concat([y_test_df.reset_index(drop=True), y_test_pred_df.reset_index(drop=True)],axis=1)\n",
    "y_predicted_final['final_predicted'] = y_predicted_final.Converting_Probability.map(lambda x: 1 if x > 0.27 else 0)\n",
    "y_predicted_final['Lead_Score'] = y_predicted_final.Converting_Probability.map(lambda x: round(x*100))\n",
    "\n",
    "y_predicted_final.head()\n",
    "\n",
    "# Evaluating model performance on test data\n",
    "\n",
    "evaluate_model(y_predicted_final.Converted, y_predicted_final.final_predicted, print_score=True)\n",
    "\n",
    "# Final model\n",
    "\n",
    "def build_model_cutoff(X_train, y_train, X_test, y_test, cutoff=0.5):\n",
    "    \n",
    "    # Train model\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    logm = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "    res = logm.fit()\n",
    "\n",
    "    y_train_pred = res.predict(X_train_sm).values.reshape(-1)\n",
    "    \n",
    "    y_train_pred_final = pd.DataFrame({'Prospect ID':y_train.index, 'Converted':y_train.values, 'Convert_Probability':y_train_pred})\n",
    "    y_train_pred_final['Convert_predicted'] = y_train_pred_final.Convert_Probability.map(lambda x: 1 if x > cutoff else 0)\n",
    "    y_train_pred_final['Lead_Score'] = y_train_pred_final.Convert_Probability.map(lambda x: round(x*100))\n",
    "    print(\"------------------Result of training data-------------------\")\n",
    "    print(y_train_pred_final.head())\n",
    "    \n",
    "    # Predicting Lead Score on Test data\n",
    "    X_test_sm = sm.add_constant(X_test)\n",
    "    y_test_pred = res.predict(X_test_sm)\n",
    "\n",
    "    y_test_pred_final = pd.DataFrame({'Prospect ID':y_test.index, 'Converted':y_test.values, 'Convert_Probability':y_test_pred})\n",
    "    y_test_pred_final['Convert_predicted'] = y_test_pred_final.Convert_Probability.map(lambda x: 1 if x > cutoff else 0)\n",
    "    y_test_pred_final['Lead_Score'] = y_test_pred_final.Convert_Probability.map(lambda x: round(x*100))\n",
    "    y_test_pred_final.reset_index(inplace=True, drop=True)\n",
    "    print(\"------------------Result of test data-------------------\")\n",
    "    print(y_test_pred_final.head())\n",
    "    \n",
    "    print(\"------------------Model Evaluation Metrics-------------------\")\n",
    "    evaluate_model(y_test_pred_final.Converted, y_test_pred_final.Convert_predicted, print_score=True)\n",
    "    \n",
    "    return y_test_pred_final\n",
    "\n",
    "build_model_cutoff(X_train[col], y_train, X_test[col], y_test, cutoff=0.27)\n",
    "\n",
    "print(\"Features used in Final Model :\", col)\n",
    "\n",
    "print(\"-----------------------Feature Importance--------------------\")\n",
    "print(res.params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
